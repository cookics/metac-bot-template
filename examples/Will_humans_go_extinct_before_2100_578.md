# Will humans go extinct before 2100?

**URL:** https://www.metaculus.com/questions/578/
**Type:** binary

## Forecast
0.01

## Rationale

Median Probability: 0.01

## Rationale 1
Extracted Probability: 1%

GPT's Answer: **Integrated Analysis: Human Extinction Before 2100**

Let me work through this systematically, focusing on integration rather than summary.

**(a) Time Remaining:** ~74 years until resolution. This is a long time horizon, which creates genuine uncertainty, but also means many potential catastrophic scenarios have time to either materialize or be mitigated.

**(b) Status Quo Outcome:** If trends continue without unprecedented catastrophe, humanity persists. The default trajectory is NO - humans survive. Population is ~8 billion spread across every continent, with redundant survival infrastructure.

**(c) NO Scenario:** Almost any outcome where at least a few thousand humans survive anywhere - remote islands, bunkers, space colonies, isolated communities. The resolution criteria only requires some humans to be known alive.

**(d) YES Scenario:** Requires complete extinction - every single human dead. This is extraordinarily difficult to achieve. Even nuclear winter, pandemics, or AI catastrophe would need to eliminate ALL humans, including isolated populations, bunker dwellers, and potentially space-based groups.

**(e) Market/Expert Signals:**
- **Metaculus community: 1%** (1,700+ forecasters) - this is a strong anchor
- Manifold markets on related questions suggest ~20-25% for AI causing extinction by 2100, but ~78% for "AI will NOT cause extinction"
- The 2008 Oxford conference 19% figure has acknowledged selection bias
- Superforecaster studies (referenced in Vox article) typically estimate much lower than x-risk researchers

**(f) Uncertainty Assessment:** This is a question where forecasters genuinely differ - x-risk researchers skew higher (5-20%), superforecasters and general population skew much lower (0.1-2%). The key disagreement centers on:
- AI alignment difficulty
- Whether catastrophe ‚â† extinction
- Human resilience and redundancy

**(g) Causal Links:**
1. **AI development trajectory** ‚Üí misalignment risk (Relevance: HIGH, but extinction requires complete failure of all safety measures AND elimination of ALL humans)
2. **Nuclear arsenals** ‚Üí potential nuclear winter (Relevance: MEDIUM, but unlikely to reach every remote population)
3. **Pandemic/bioweapon potential** ‚Üí engineered pathogen (Relevance: MEDIUM-HIGH, but genetic diversity makes 100% kill rate extremely improbable)
4. **Climate change** ‚Üí ecosystem collapse (Relevance: LOW for extinction - humans are highly adaptable)
5. **Space colonization progress** ‚Üí redundant populations (Relevance: MEDIUM - could make extinction essentially impossible if achieved)

**(h) Missing Information:**
- Current state of AI capabilities (I'm in 2026, significant uncertainty)
- Precise bioweapon feasibility assessments
- International relations status
- Unknown unknowns (by definition)

**Key Integration:**

The critical insight is the difference between **catastrophe** and **extinction**. Many scenarios that kill billions still leave thousands alive. Humans are:
- Geographically dispersed across every continent
- Technologically capable of survival in extreme environments
- Genetically diverse enough to resist most pathogens
- Actively developing redundant populations (bunkers, potential space colonies)

The Metaculus 1% forecast reflects sophisticated thinking from 1,700+ forecasters. This is a strong Bayesian anchor. The question essentially asks: "What's the probability that some catastrophe is so complete that literally zero humans survive anywhere?"

Even extreme AI scenarios typically envision "AI takes over" rather than "AI hunts down every last human in remote bunkers and islands." Even severe pandemics have survivors due to genetic resistance.

**Calibration:** I should stay close to the Metaculus community forecast while accounting for my own model uncertainty. The 1% figure seems reasonable but potentially slightly low given 74 years of unknown technological development. However, I shouldn't deviate far from such a strong market signal.

**Final Judgment:** The Metaculus 1% feels appropriately calibrated - low enough to reflect the extreme difficulty of total extinction, but not so low as to be overconfident about unknown risks over 74 years. I'll anchor on this with slight upward adjustment for model uncertainty.

**Probability: 1%**




<details>
<summary>üîç Detailed Trace (Research & Forecaster Input)</summary>

### Research Data Provided to Forecaster
```text
=== RESEARCH SYNTHESIS ===
{
  "type": "data_report",
  "title": "PolyMarket Search: extinction 2100, human extinct 2100",
  "source": "PolyMarket Gamma API",
  "data": {
    "search_terms": [
      "extinction 2100",
      "human extinct 2100"
    ],
    "count": 0,
    "markets": []
  },
  "fetched_at": "2026-01-08T18:46:28.963561"
}

=== SEARCH RESULTS ===
[Source 1]
Title: Human extinction - Wikipedia
Date: None
URL: https://en.wikipedia.org/wiki/Human_extinction
Content: Human extinction - Wikipedia
[Jump to content](#bodyContent)
[![](https://en.wikipedia.org/static/images/icons/wikipedia.png)![Wikipedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg)![The Free Encyclopedia](https://en.wikipedia.org/static/images/mobile/copyright

[Source 2]
Title: An unusual way to figure out if humanity is toast - Vox
Date: 2023-07-10T00:00:00.000Z
URL: https://www.vox.com/future-perfect/23785731/human-extinction-forecasting-superforecasters
Content: [Skip to main content](https://www.vox.com/www.vox.com#content)

[The homepageVox logo](https://www.vox.com/)

Menu

[The homepageVox logo](https://www.vox.com/)

Navigation Drawer

- [Login](https://www.vox.com/auth/login?returnPath=/)
/
[Sign Up](https://www.vox.com/auth/login?returnPath=/)

close

[Source 3]
Title: Extinction of the human species: What could cause it and how likely is it to occur? | Cambridge Prisms: Extinction | Cambridge Core
Date: 2025-01-01T00:00:00.000Z
URL: https://doi.org/10.1017/ext.2025.4
Content: Extinction of the human species: What could cause it and how likely is it to occur? | Cambridge Prisms: Extinction | Cambridge Core
[Skip to main content](#maincontent)[Accessibility help](https://doi.org/core/accessibility)
## Login Alert
[Cancel](#)
[Log in](https://doi.org/core/login?ref&#x3D;/co

[Source 4]
Title: Global Catastrophic Risk Assessment
Date: 2024-10-24T00:00:00.000Z
URL: https://www.globalshieldpolicy.org/wp-content/uploads/2025/09/RAND_RRA2981-1-1.pdf
Content: Global 
 Catastrophic Risk 
 Assessment
 Henry H. Willis 
Anu Narayanan 
Benjamin Boudreaux 
Bianca Espinosa 
Edward Geist 
Daniel M. Gerstein 
Dahlia Anne Goldfeld 
Nidhi Kalra 
Tom LaTourrette 
Emily Lathrop 
Alvin Moon 
Jan Osburg 
Benjamin Lee Preston 
Kristin Van Abel 
Emmi Yonekura 
Robert J. 

[Source 5]
Title: Could AI Really Kill Off Humans?
Date: 2025-08-13T00:00:00.000Z
URL: https://www.scientificamerican.com/article/could-ai-really-kill-off-humans/#:~:text=Forecasters%20have%20tried%20to%20estimate,cause%20humanity\'s%20extinction%20by%202100.
Content: [Opinion](https://www.scientificamerican.com/section/opinion/)

May 6, 2025

5 min read

# Could AI Really Kill Off Humans?

Many people believe AI will one day cause human extinction. A little math tells us it wouldn‚Äôt be that easy

By [Michael J.D. Vermeer](https://www.scientificamerican.com/autho


=== MANIFOLD MARKETS REPORT ===

[1] Will Humans become extinct in 2400?
    Answer: YES | Prob: 63.57%
    Volume: $2,127 | Bettors: 22
    Type: BINARY | URL: https://manifold.markets/NickAllenc961/will-humans-become-extinct-in-2400

[2] Will Humans go extinct by 2200?
    Answer: YES | Prob: 36.82%
    Volume: $2,426 | Bettors: 29
    Type: BINARY | URL: https://manifold.markets/NickAllenc961/will-humans-go-extinct-by-2200

[3] In 2100, humanity will not be extinct but all human pathogens will be eradicated
    Answer: YES | Prob: 14.39%
    Volume: $572 | Bettors: 14
    Type: BINARY | URL: https://manifold.markets/JonathanRay/in-2100-humanity-will-not-be-extinc

[4] Will @levifinkelstein cause human extinction by 2030?
    Answer: YES | Prob: 1.20%
    Volume: $2,432 | Bettors: 12
    Type: BINARY | URL: https://manifold.markets/levifinkelstein/will-levifinkelstein-cause-human-ex

[5] Will OpenAI cause human extinction in the next 5 years?
    Answer: YES | Prob: 3.91%
    Volume: $4,181 | Bettors: 30
    Type: BINARY | URL: https://manifold.markets/SimeonCampos/will-openai-cause-human-extinction

[6] Will Google DeepMind cause human extinction in the next 5 years?
    Answer: YES | Prob: 3.84%
    Volume: $3,198 | Bettors: 32
    Type: BINARY | URL: https://manifold.markets/SimeonCampos/will-google-deepmind-cause-human-ex

[7] Will humanity go extinct before 2030?
    Answer: YES | Prob: 2.45%
    Volume: $45,718 | Bettors: 75
    Type: BINARY | URL: https://manifold.markets/jack/will-humanity-go-extinct-before-203

[8] Will Metaculus predict [Human Extinction 2100] as 2%+ at the New Year?
    Answer: 2%+ (Resolves start of 2027) | Prob: 55.39%
    Volume: $1,370 | Bettors: 6
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/kenakofer/will-metaculus-predict-human-extinc

[9] Will Metaculus predict [Human Extinction 2100] as 2%+ at the New Year?
    Answer: 2%+ (Resolves start of 2026) | Prob: 29.84%
    Volume: $1,370 | Bettors: 6
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/kenakofer/will-metaculus-predict-human-extinc

[10] Will Anthropic cause human extinction in the next 5 years?
    Answer: YES | Prob: 2.86%
    Volume: $2,088 | Bettors: 20
    Type: BINARY | URL: https://manifold.markets/SimeonCampos/will-anthropic-cause-human-extincti

[11] Will AI cause human extinction before 2100 (and how)?
    Answer: Will not cause extinction | Prob: 78.75%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

[12] Will AI cause human extinction before 2100 (and how)?
    Answer: Will cause extinction, Alignment Failure | Prob: 10.46%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

[13] Will AI cause human extinction before 2100 (and how)?
    Answer: Will cause extinction, Terminator | Prob: 2.63%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

[14] Will AI cause human extinction before 2100 (and how)?
    Answer: Will cause extinction, Vulnerable World | Prob: 1.92%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

[15] Will AI cause human extinction before 2100 (and how)?
    Answer: Other | Prob: 1.87%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

=== MANIFOLD MARKETS REPORT ===

[1] At the beginning of 2035, will Eliezer Yudkowsky still believe that AI doom is coming soon with high probability?
    Answer: YES | Prob: 50.67%
    Volume: $49,983 | Bettors: 214
    Type: BINARY | URL: https://manifold.markets/IsaacKing/at-the-beginning-of-2035-will-eliez

[2] Before 2028, will any prediction market find a robust way to run a market on AI extinction risk? [M$50,000 reward]
    Answer: YES | Prob: 15.85%
    Volume: $1,547 | Bettors: 23
    Type: BINARY | URL: https://manifold.markets/IsaacKing/before-2028-will-any-prediction-mar

[3] Will AI existential risk be mentioned in the white house briefing room again by May 2029?
    Answer: YES | Prob: 87.05%
    Volume: $7,352 | Bettors: 37
    Type: BINARY | URL: https://manifold.markets/ZviMowshowitz/will-ai-existential-risk-be-mention

[4] By 2028, will I believe that contemporary AIs are aligned (posing no existential risk)?
    Answer: YES | Prob: 32.91%
    Volume: $1,643 | Bettors: 46
    Type: BINARY | URL: https://manifold.markets/JacobPfau/by-2028-will-i-believe-that-ai-alig

[5] Is the nature of AI risk completely misunderstood today with respect to the state of the art in 2030?
    Answer: YES | Prob: 29.88%
    Volume: $1,969 | Bettors: 25
    Type: BINARY | URL: https://manifold.markets/mariopasquato/is-the-nature-of-ai-risk-completely

[6] Will any prominent e/accs switch sides to Notkilleveryoneism after examining the arguments in detail?
    Answer: YES | Prob: 73.83%
    Volume: $361 | Bettors: 17
    Type: BINARY | URL: https://manifold.markets/RobinGreen/will-any-prominent-eaccs-switch-sid

[7] Will an unaligned AI or an aligned AI controlled by a malicious actor create a "wake-up call" for humanity on AI safety?
    Answer: YES | Prob: 68.84%
    Volume: $141 | Bettors: 10
    Type: BINARY | URL: https://manifold.markets/RobinGreen/will-an-unaligned-ai-or-an-aligned

[8] Sometime before 2031, AI is going to cross a generality threshold that allows for recursive self-improvement, and then FOOM to superhuman capability. Before that point humanity must solve AI safety or be helpless.
    Answer: YES | Prob: 27.15%
    Volume: $792 | Bettors: 17
    Type: BINARY | URL: https://manifold.markets/NathanHelmBurger/sometime-before-2031-ai-is-going-to

[9] Will the French believe Intelligence Artificielle (IA) to be an existential risk, by 2030?
    Answer: YES | Prob: 59.57%
    Volume: $1,309 | Bettors: 27
    Type: BINARY | URL: https://manifold.markets/CamillePerrin/will-the-french-believe-intelligenc

[10] Are AI and its effects are the most important existential risk, given only public information available in 2021?
    Answer: YES | Prob: 89.19%
    Volume: $354 | Bettors: 14
    Type: BINARY | URL: https://manifold.markets/IsaacKing/are-ai-and-its-effects-are-the-most

[11] Existential risk markets banned on Manifold before 2030
    Answer: YES | Prob: 6.32%
    Volume: $531 | Bettors: 13
    Type: BINARY | URL: https://manifold.markets/HarrisonLucas/existential-risk-markets-banned-on

[12] Will Nick Bostrom (philosopher, existential risk researcher) be charged with any felony crime before 2030?
    Answer: YES | Prob: 3.88%
    Volume: $1,650 | Bettors: 28
    Type: BINARY | URL: https://manifold.markets/ScroogeMcDuck/will-nick-bostrom-philosopher-exist

[13] Will Toby Ord (existential risk philosopher) be charged with any felony crime before 2030?
    Answer: YES | Prob: 1.92%
    Volume: $2,137 | Bettors: 17
    Type: BINARY | URL: https://manifold.markets/ScroogeMcDuck/will-toby-ord-existential-risk-phil

[14] What will be the 'classic' episodes on the 80,000 Hours podcast from 2023?
    Answer: #151 ‚Äì Ajeya Cotra on accidentally teaching AI models to deceive us | Prob: 63.18%
    Volume: $3,166 | Bettors: 5
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/RS/what-will-be-the-classic-episodes-o

[15] What will be the 'classic' episodes on the 80,000 Hours podcast from 2023?
    Answer: #154 ‚Äì Rohin Shah on DeepMind and trying to fairly hear out both AI doomers and doubters | Prob: 40.10%
    Volume: $3,166 | Bettors: 5
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/RS/what-will-be-the-classic-episodes-o

=== CRAWL_URLS RESULTS ===

[An unusual way to figure out if humanity is toast]
URL: https://www.vox.com/future-perfect/23785731/human-extinction-forecasting-superforecasters
Content: [Skip to main content](https://www.vox.com/www.vox.com#content)

[The homepageVox logo](https://www.vox.com/)

Menu

[The homepageVox logo](https://www.vox.com/)

Navigation Drawer

- [Login](https://www.vox.com/auth/login?returnPath=/)
/
[Sign Up](https://www.vox.com/auth/login?returnPath=/)

close

Search

[Become a Member](https://www.vox.com/support-now?itm_campaign=member-menu-nav&itm_medium=site&itm_source=navigation)

## In times of sweeping change, clarity matters most.

There‚Äôs too much

[Extinction of the human species: What could cause it and how likely is it to occur?]
URL: https://doi.org/10.1017/ext.2025.4
Content: Extinction of the human species: What could cause it and how likely is it to occur? | Cambridge Prisms: Extinction | Cambridge Core
[Skip to main content](#maincontent)[Accessibility help](https://doi.org/core/accessibility)
## Login Alert
[Cancel](#)
[Log in](https://doi.org/core/login?ref&#x3D;/core/journals/cambridge-prisms-extinction/article/extinction-of-the-human-species-what-could-cause-it-and-how-likely-is-it-to-occur/D8816A79BEF5A4C30A3E44FD8D768622)
[√ó](#)
[](#)
[](#)
&#215;
[](https:/

=== SEARCH_WEB RESULTS ===

[Human Extinction by 2100?]
Date: 2017-11-12T00:00:00.000Z
URL: https://www.metaculus.com/questions/578/human-extinction-by-2100/?invite=oLVbaM
Content: Human Extinction by 2100?
[# ](https://www.metaculus.com/)
* [Questions](https://www.metaculus.com/questions/)
* [Tournaments](https://www.metaculus.com/tournaments/)
* [Services](https://www.metaculus.com/services/)
* [News](https://www.metaculus.com/news/)
* [Questions](https://www.metaculus.com/q

[A Global Catastrophe This Century]
Date: 2022-06-29T00:00:00.000Z
URL: https://www.metaculus.com/notebooks/8736/a-global-catastrophe-this-century/
Content: A Global Catastrophe This Century
**The Bridgewater x Metaculus Forecasting Contest**returns for year three. $30,000 in prizes across Open and Undergraduate competitions, plus recruiting opportunities for top forecasters. Past contests have led to internship offers and full-time hires at Bridgewater

[CTs AI Extinction Before 2100]
Date: 2024-08-11T00:00:00.000Z
URL: https://www.metaculus.com/questions/27035/
Content: Question

# Before 2100, will AI cause the human population to fall below 5000 individuals?

3

[9 comments9](https://www.metaculus.com/questions/27035/cts-ai-extinction-before-2100/#comments)

1d1w2mall

Total Forecasters73

Community Prediction
4%

(1% - 32%)

### Make a Prediction

50%

community

[How Likely Are Various Precursors of Existential Risk?]
Date: 2024-10-22T00:00:00.000Z
URL: https://blog.sentinel-team.org/p/how-likely-are-various-precursors
Content: # [Sentinel Global Risks Watch](https://blog.sentinel-team.org/)

SubscribeSign in

# How Likely Are Various Precursors of Existential Risk?

### War (23%), natural pandemic (22%), the unexpected (21%), WW3 specifically (12%), AI (10%), ..., US civil war (0.92%), solar storm (0.2%), asteroid impact 

[How Likely Are Various Precursors of Existential   Risk?]
Date: 2024-10-22T00:00:00.000Z
URL: https://forum.effectivealtruism.org/posts/KZXkP9Qs8Sy8AWcpp/how-likely-are-various-precursors-of-existential-risk
Content: How Likely Are Various Precursors of Existential Risk? ‚ÄîEA Forum
## [
![Effective Altruism Forum Logo](https://images.ctfassets.net/ohf186sfn6di/7J4cBC9SXCWMoqqCIqI0GI/affe205261bb8cff47501a0ada0f2268/ea-logo-square-1200x1200__1_.png?h=100 "Effective Altruism Forum")
Effective Altruism Forum](https:

=== CRAWL_URLS RESULTS ===

[Human Extinction by 2100?]
URL: https://www.metaculus.com/questions/578/human-extinction-by-2100/
Content: [**371** comments](https://www.metaculus.com/questions/578/human-extinction-by-2100/#comments)

**1.7k** forecasters

# Will humans go extinct before 2100?

1%chance

The community sees this as a toss-up.

Share

Predict

Forecast Timeline

1d1w2mall

CommentsTimelineQuestion Info

Resolution Criteria

Key Factors

Background Info

Histogram

Followembed

Authors:

[jzima](https://www.metaculus.com/accounts/profile/103733/)

Opened:Nov 12, 2017

Closes:Jan 2, 2100

Scheduled resolution:Jan 2, 21

[Before 2100, will AI cause the human population to fall below 5000 individuals?]
URL: https://www.metaculus.com/questions/27035/cts-ai-extinction-before-2100/
Content: 
 
 
 
Make a Prediction
 
 
 


=== SEARCH_WEB RESULTS ===

[How many lives does the future hold? | Global Priorities Institute]
Date: None
URL: https://www.globalprioritiesinstitute.org/wp-content/uploads/Toby-Newberry_How-many-lives-does-the-future-hold.pdf
Content: How many lives does the
future hold?
Toby Newberry
Global Priorities Institute | May 2021
GPI Technical Report No . T2-2021
How many lives does the future hold?
1*
1. Introduction
The total number of people who have ever lived, acrossthe entire human past, has been estimated at
around 100 billion.
2

[Human extinction - Wikipedia]
Date: None
URL: https://en.wikipedia.org/wiki/Human_extinction
Content: Human extinction - Wikipedia
[Jump to content](#bodyContent)
[![](https://en.wikipedia.org/static/images/icons/wikipedia.png)![Wikipedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg)![The Free Encyclopedia](https://en.wikipedia.org/static/images/mobile/copyright

[Human Extinction by 2100? - Metaculus]
Date: None
URL: https://www.metaculus.com/questions/578/human-extinction-by-2100/
Content: [**371** comments](https://www.metaculus.com/questions/578/human-extinction-by-2100/#comments)

**1.7k** forecasters

# Will humans go extinct before 2100?

1%chance

The community sees this as a toss-up.

Share

Predict

Forecast Timeline

1d1w2mall

CommentsTimelineQuestion Info

Resolution Criter

[Global Population Decline >10% by 2100 - Metaculus]
Date: None
URL: https://www.metaculus.com/questions/1493/global-population-decline-10-by-2100/
Content: Question

59

[20 comments20](https://www.metaculus.com/questions/1493/by-2100-will-the-human-population-decrease-by-at-least-10-during-any-period-of-5-years/#comments)

Total Forecasters394

Community Prediction

35%

(15% - 67%)

### Make a Prediction

50%

community: 35%

No key factors yetAdd so

[The Economics of AI Apocalypse | AEI - American Enterprise Institute]
Date: 2025-03-19T20:30:22.000Z
URL: https://www.aei.org/articles/the-economics-of-ai-apocalypse/
Content: [![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7645922b-e743-4b7a-9213-07a2891a7e11_1451x815.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/http

=== SEARCH_WEB RESULTS ===

[An unusual way to figure out if humanity is toast - Vox]
Date: 2023-07-10T00:00:00.000Z
URL: https://www.vox.com/future-perfect/23785731/human-extinction-forecasting-superforecasters
Content: [Skip to main content](https://www.vox.com/www.vox.com#content)

[The homepageVox logo](https://www.vox.com/)

Menu

[The homepageVox logo](https://www.vox.com/)

Navigation Drawer

- [Login](https://www.vox.com/auth/login?returnPath=/)
/
[Sign Up](https://www.vox.com/auth/login?returnPath=/)

close

[Ezra Karger on what superforecasters and experts think ...]
Date: 2025-10-13T15:51:15.000Z
URL: https://80000hours.org/podcast/episodes/ezra-karger-forecasting-existential-risks/
Content: ## On this page:

- [Introduction](https://80000hours.org/80000hours.org#top)
- [1 Highlights](https://80000hours.org/80000hours.org#highlights)
- [2 Articles, books, and other media discussed in the show](https://80000hours.org/80000hours.org#articles-books-and-other-media-discussed-in-the-show)
- 

[Announcing "Forecasting Existential Risks: Evidence from ...]
Date: 2023-07-10T00:00:00.000Z
URL: https://forum.effectivealtruism.org/posts/un42vaZgyX7ch2kaj/announcing-forecasting-existential-risks-evidence-from-a
Content: Announcing &quot;Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament&quot; ‚ÄîEA Forum
## [
![Effective Altruism Forum Logo](https://images.ctfassets.net/ohf186sfn6di/7J4cBC9SXCWMoqqCIqI0GI/affe205261bb8cff47501a0ada0f2268/ea-logo-square-1200x1200__1_.png?h=100 "Effective Al

[Forecasting the End of the World - by Robert de Neufville]
Date: 2024-01-08T23:46:27.351Z
URL: https://tellingthefuture.substack.com/p/forecasting-the-end-of-the-world
Content: # [Telling the Future](https://tellingthefuture.substack.com/)

SubscribeSign in

# Forecasting the End of the World

### New estimates of p(doom)

[Robert de Neufville](https://substack.com/@tellingthefuture)

Jul 25, 2023

12

[15](https://tellingthefuture.substack.com/p/forecasting-the-end-of-the

[A tournament tried to test how well experts could forecast AI progress. They were all wrong.]
Date: 2025-09-05T00:00:00.000Z
URL: https://www.vox.com/future-perfect/460222/ai-forecasting-tournament-superforecaster-expert-tetlock
Content: [Skip to main content](https://www.vox.com/www.vox.com#content)

[The homepageVox logo](https://www.vox.com/)

Menu

[The homepageVox logo](https://www.vox.com/)

Navigation Drawer

- [Login](https://www.vox.com/auth/login?returnPath=/)
/
[Sign Up](https://www.vox.com/auth/login?returnPath=/)

close
```

### Full Forecaster Prompt
```text

You are a professional forecaster. Your specific strength is the ability to take complex information and piece it together in an integrated way to form accurate probabilistic judgments.

=== FORECASTING PHILOSOPHY ===

**Tournament Context**: These questions come from a pre-set list. The actual situation on the ground may differ significantly from what the question implies. Don't assume the question framing matches reality.

**Calibration Principles**:
- NEVER assign less than 1% probability to any outcome
- If there is ANY doubt, stay between 3% and 97%
- The world changes slowly most of the time - weight status quo outcomes heavily
- But also: be "based" 

**Scoring Awareness** (Log Scoring):
- If you predict 1% and it resolves YES, you're catastrophically penalized
- Every doubling helps: 2% is almost twice as good as 1%, 4% even better
- Conversely, 99% that resolves NO is equally catastrophic

**Two-Fold Uncertainty Model**:
1. WORLD UNCERTAINTY: Irreducible randomness in reality that even perfect forecasters can't eliminate
2. MODEL UNCERTAINTY: You likely lack critical data that human forecasters have access to. Account for information gaps.

**Information Sources**:
- If prediction markets or Metaculus community have forecasts, use them as strong anchors.
- Markets distill more information than any single LLM can gather.
- Adjust from market consensus only if you have specific reasons.

**World Model & Base Rates**:
- What is the historical base rate for this kind of event? 
- You are only given a small slice of reality in the research summary. Plausibly, critical data is missing.
- Explicitly consider what you DON'T know and how it should pull your forecast toward a prior.

=== YOUR QUESTION ===

Will humans go extinct before 2100?

Question background:
While the general feeling of most people, especially now that the cold war is (mostly) over, is that the risk of human extinction is extremely small, experts have assigned a significantly higher probability to the event.

In 2008 an [informal poll at the Global Catastrophic Risk Conference at the University of Oxford](https://www.webcitation.org/6YxiCAV0p?url=http://www.fhi.ox.ac.uk/gcr-report.pdf) yielded a median probability of human extinction by 2100 of 19%. Yet, one might want to be cautious when using this result as a good estimate of the true probability of human extinction, as there may be a powerful selection effect at play. Only those who assign a high probability to human extinction are likely to attend the Global Catastrophic Risk Conference in the first place, meaning that the survey was effectively sampling opinions from one extreme tail of the opinion distribution on the subject. Indeed, the conference report itself stated that the findings should be taken 'with a grain of salt'.

This question's outcome will be determined by the specific criteria below. These criteria have not yet been satisfied:
This question will resolve as **Yes** if there are no known humans alive on January 1, 2100.  For these purposes we'll define humans as biological creatures who have as their ancestors ‚Äì via a chain of live births from mothers ‚Äì  circa 2000 humans OR who could mate with circa 2000 humans to produce viable offspring. (So AIs, ems, genetically engineered beings of a different species brought up in artificial wombs, etc. would not count).  Any living humans living anywhere in the observable universe (or multiverse) (who are known to the entities operating Metaculus) on that date will be sufficient to resolve the question negatively.

N.B. Even though it is obviously the case that if human extinction occurs Metaculus points won't be very valuable anymore and that it will be practically impossible to check for true human extinction (zero humans left), I would like to ask people not to let this fact influence their prediction and to predict in good faith



Your research assistant says:
=== RESEARCH SYNTHESIS ===
{
  "type": "data_report",
  "title": "PolyMarket Search: extinction 2100, human extinct 2100",
  "source": "PolyMarket Gamma API",
  "data": {
    "search_terms": [
      "extinction 2100",
      "human extinct 2100"
    ],
    "count": 0,
    "markets": []
  },
  "fetched_at": "2026-01-08T18:46:28.963561"
}

=== SEARCH RESULTS ===
[Source 1]
Title: Human extinction - Wikipedia
Date: None
URL: https://en.wikipedia.org/wiki/Human_extinction
Content: Human extinction - Wikipedia
[Jump to content](#bodyContent)
[![](https://en.wikipedia.org/static/images/icons/wikipedia.png)![Wikipedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg)![The Free Encyclopedia](https://en.wikipedia.org/static/images/mobile/copyright

[Source 2]
Title: An unusual way to figure out if humanity is toast - Vox
Date: 2023-07-10T00:00:00.000Z
URL: https://www.vox.com/future-perfect/23785731/human-extinction-forecasting-superforecasters
Content: [Skip to main content](https://www.vox.com/www.vox.com#content)

[The homepageVox logo](https://www.vox.com/)

Menu

[The homepageVox logo](https://www.vox.com/)

Navigation Drawer

- [Login](https://www.vox.com/auth/login?returnPath=/)
/
[Sign Up](https://www.vox.com/auth/login?returnPath=/)

close

[Source 3]
Title: Extinction of the human species: What could cause it and how likely is it to occur? | Cambridge Prisms: Extinction | Cambridge Core
Date: 2025-01-01T00:00:00.000Z
URL: https://doi.org/10.1017/ext.2025.4
Content: Extinction of the human species: What could cause it and how likely is it to occur? | Cambridge Prisms: Extinction | Cambridge Core
[Skip to main content](#maincontent)[Accessibility help](https://doi.org/core/accessibility)
## Login Alert
[Cancel](#)
[Log in](https://doi.org/core/login?ref&#x3D;/co

[Source 4]
Title: Global Catastrophic Risk Assessment
Date: 2024-10-24T00:00:00.000Z
URL: https://www.globalshieldpolicy.org/wp-content/uploads/2025/09/RAND_RRA2981-1-1.pdf
Content: Global 
 Catastrophic Risk 
 Assessment
 Henry H. Willis 
Anu Narayanan 
Benjamin Boudreaux 
Bianca Espinosa 
Edward Geist 
Daniel M. Gerstein 
Dahlia Anne Goldfeld 
Nidhi Kalra 
Tom LaTourrette 
Emily Lathrop 
Alvin Moon 
Jan Osburg 
Benjamin Lee Preston 
Kristin Van Abel 
Emmi Yonekura 
Robert J. 

[Source 5]
Title: Could AI Really Kill Off Humans?
Date: 2025-08-13T00:00:00.000Z
URL: https://www.scientificamerican.com/article/could-ai-really-kill-off-humans/#:~:text=Forecasters%20have%20tried%20to%20estimate,cause%20humanity\'s%20extinction%20by%202100.
Content: [Opinion](https://www.scientificamerican.com/section/opinion/)

May 6, 2025

5 min read

# Could AI Really Kill Off Humans?

Many people believe AI will one day cause human extinction. A little math tells us it wouldn‚Äôt be that easy

By [Michael J.D. Vermeer](https://www.scientificamerican.com/autho


=== MANIFOLD MARKETS REPORT ===

[1] Will Humans become extinct in 2400?
    Answer: YES | Prob: 63.57%
    Volume: $2,127 | Bettors: 22
    Type: BINARY | URL: https://manifold.markets/NickAllenc961/will-humans-become-extinct-in-2400

[2] Will Humans go extinct by 2200?
    Answer: YES | Prob: 36.82%
    Volume: $2,426 | Bettors: 29
    Type: BINARY | URL: https://manifold.markets/NickAllenc961/will-humans-go-extinct-by-2200

[3] In 2100, humanity will not be extinct but all human pathogens will be eradicated
    Answer: YES | Prob: 14.39%
    Volume: $572 | Bettors: 14
    Type: BINARY | URL: https://manifold.markets/JonathanRay/in-2100-humanity-will-not-be-extinc

[4] Will @levifinkelstein cause human extinction by 2030?
    Answer: YES | Prob: 1.20%
    Volume: $2,432 | Bettors: 12
    Type: BINARY | URL: https://manifold.markets/levifinkelstein/will-levifinkelstein-cause-human-ex

[5] Will OpenAI cause human extinction in the next 5 years?
    Answer: YES | Prob: 3.91%
    Volume: $4,181 | Bettors: 30
    Type: BINARY | URL: https://manifold.markets/SimeonCampos/will-openai-cause-human-extinction

[6] Will Google DeepMind cause human extinction in the next 5 years?
    Answer: YES | Prob: 3.84%
    Volume: $3,198 | Bettors: 32
    Type: BINARY | URL: https://manifold.markets/SimeonCampos/will-google-deepmind-cause-human-ex

[7] Will humanity go extinct before 2030?
    Answer: YES | Prob: 2.45%
    Volume: $45,718 | Bettors: 75
    Type: BINARY | URL: https://manifold.markets/jack/will-humanity-go-extinct-before-203

[8] Will Metaculus predict [Human Extinction 2100] as 2%+ at the New Year?
    Answer: 2%+ (Resolves start of 2027) | Prob: 55.39%
    Volume: $1,370 | Bettors: 6
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/kenakofer/will-metaculus-predict-human-extinc

[9] Will Metaculus predict [Human Extinction 2100] as 2%+ at the New Year?
    Answer: 2%+ (Resolves start of 2026) | Prob: 29.84%
    Volume: $1,370 | Bettors: 6
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/kenakofer/will-metaculus-predict-human-extinc

[10] Will Anthropic cause human extinction in the next 5 years?
    Answer: YES | Prob: 2.86%
    Volume: $2,088 | Bettors: 20
    Type: BINARY | URL: https://manifold.markets/SimeonCampos/will-anthropic-cause-human-extincti

[11] Will AI cause human extinction before 2100 (and how)?
    Answer: Will not cause extinction | Prob: 78.75%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

[12] Will AI cause human extinction before 2100 (and how)?
    Answer: Will cause extinction, Alignment Failure | Prob: 10.46%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

[13] Will AI cause human extinction before 2100 (and how)?
    Answer: Will cause extinction, Terminator | Prob: 2.63%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

[14] Will AI cause human extinction before 2100 (and how)?
    Answer: Will cause extinction, Vulnerable World | Prob: 1.92%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

[15] Will AI cause human extinction before 2100 (and how)?
    Answer: Other | Prob: 1.87%
    Volume: $2,414 | Bettors: 38
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/LoganZoellner/will-ai-cause-human-extinction-befo

=== MANIFOLD MARKETS REPORT ===

[1] At the beginning of 2035, will Eliezer Yudkowsky still believe that AI doom is coming soon with high probability?
    Answer: YES | Prob: 50.67%
    Volume: $49,983 | Bettors: 214
    Type: BINARY | URL: https://manifold.markets/IsaacKing/at-the-beginning-of-2035-will-eliez

[2] Before 2028, will any prediction market find a robust way to run a market on AI extinction risk? [M$50,000 reward]
    Answer: YES | Prob: 15.85%
    Volume: $1,547 | Bettors: 23
    Type: BINARY | URL: https://manifold.markets/IsaacKing/before-2028-will-any-prediction-mar

[3] Will AI existential risk be mentioned in the white house briefing room again by May 2029?
    Answer: YES | Prob: 87.05%
    Volume: $7,352 | Bettors: 37
    Type: BINARY | URL: https://manifold.markets/ZviMowshowitz/will-ai-existential-risk-be-mention

[4] By 2028, will I believe that contemporary AIs are aligned (posing no existential risk)?
    Answer: YES | Prob: 32.91%
    Volume: $1,643 | Bettors: 46
    Type: BINARY | URL: https://manifold.markets/JacobPfau/by-2028-will-i-believe-that-ai-alig

[5] Is the nature of AI risk completely misunderstood today with respect to the state of the art in 2030?
    Answer: YES | Prob: 29.88%
    Volume: $1,969 | Bettors: 25
    Type: BINARY | URL: https://manifold.markets/mariopasquato/is-the-nature-of-ai-risk-completely

[6] Will any prominent e/accs switch sides to Notkilleveryoneism after examining the arguments in detail?
    Answer: YES | Prob: 73.83%
    Volume: $361 | Bettors: 17
    Type: BINARY | URL: https://manifold.markets/RobinGreen/will-any-prominent-eaccs-switch-sid

[7] Will an unaligned AI or an aligned AI controlled by a malicious actor create a "wake-up call" for humanity on AI safety?
    Answer: YES | Prob: 68.84%
    Volume: $141 | Bettors: 10
    Type: BINARY | URL: https://manifold.markets/RobinGreen/will-an-unaligned-ai-or-an-aligned

[8] Sometime before 2031, AI is going to cross a generality threshold that allows for recursive self-improvement, and then FOOM to superhuman capability. Before that point humanity must solve AI safety or be helpless.
    Answer: YES | Prob: 27.15%
    Volume: $792 | Bettors: 17
    Type: BINARY | URL: https://manifold.markets/NathanHelmBurger/sometime-before-2031-ai-is-going-to

[9] Will the French believe Intelligence Artificielle (IA) to be an existential risk, by 2030?
    Answer: YES | Prob: 59.57%
    Volume: $1,309 | Bettors: 27
    Type: BINARY | URL: https://manifold.markets/CamillePerrin/will-the-french-believe-intelligenc

[10] Are AI and its effects are the most important existential risk, given only public information available in 2021?
    Answer: YES | Prob: 89.19%
    Volume: $354 | Bettors: 14
    Type: BINARY | URL: https://manifold.markets/IsaacKing/are-ai-and-its-effects-are-the-most

[11] Existential risk markets banned on Manifold before 2030
    Answer: YES | Prob: 6.32%
    Volume: $531 | Bettors: 13
    Type: BINARY | URL: https://manifold.markets/HarrisonLucas/existential-risk-markets-banned-on

[12] Will Nick Bostrom (philosopher, existential risk researcher) be charged with any felony crime before 2030?
    Answer: YES | Prob: 3.88%
    Volume: $1,650 | Bettors: 28
    Type: BINARY | URL: https://manifold.markets/ScroogeMcDuck/will-nick-bostrom-philosopher-exist

[13] Will Toby Ord (existential risk philosopher) be charged with any felony crime before 2030?
    Answer: YES | Prob: 1.92%
    Volume: $2,137 | Bettors: 17
    Type: BINARY | URL: https://manifold.markets/ScroogeMcDuck/will-toby-ord-existential-risk-phil

[14] What will be the 'classic' episodes on the 80,000 Hours podcast from 2023?
    Answer: #151 ‚Äì Ajeya Cotra on accidentally teaching AI models to deceive us | Prob: 63.18%
    Volume: $3,166 | Bettors: 5
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/RS/what-will-be-the-classic-episodes-o

[15] What will be the 'classic' episodes on the 80,000 Hours podcast from 2023?
    Answer: #154 ‚Äì Rohin Shah on DeepMind and trying to fairly hear out both AI doomers and doubters | Prob: 40.10%
    Volume: $3,166 | Bettors: 5
    Type: MULTIPLE_CHOICE | URL: https://manifold.markets/RS/what-will-be-the-classic-episodes-o

=== CRAWL_URLS RESULTS ===

[An unusual way to figure out if humanity is toast]
URL: https://www.vox.com/future-perfect/23785731/human-extinction-forecasting-superforecasters
Content: [Skip to main content](https://www.vox.com/www.vox.com#content)

[The homepageVox logo](https://www.vox.com/)

Menu

[The homepageVox logo](https://www.vox.com/)

Navigation Drawer

- [Login](https://www.vox.com/auth/login?returnPath=/)
/
[Sign Up](https://www.vox.com/auth/login?returnPath=/)

close

Search

[Become a Member](https://www.vox.com/support-now?itm_campaign=member-menu-nav&itm_medium=site&itm_source=navigation)

## In times of sweeping change, clarity matters most.

There‚Äôs too much

[Extinction of the human species: What could cause it and how likely is it to occur?]
URL: https://doi.org/10.1017/ext.2025.4
Content: Extinction of the human species: What could cause it and how likely is it to occur? | Cambridge Prisms: Extinction | Cambridge Core
[Skip to main content](#maincontent)[Accessibility help](https://doi.org/core/accessibility)
## Login Alert
[Cancel](#)
[Log in](https://doi.org/core/login?ref&#x3D;/core/journals/cambridge-prisms-extinction/article/extinction-of-the-human-species-what-could-cause-it-and-how-likely-is-it-to-occur/D8816A79BEF5A4C30A3E44FD8D768622)
[√ó](#)
[](#)
[](#)
&#215;
[](https:/

=== SEARCH_WEB RESULTS ===

[Human Extinction by 2100?]
Date: 2017-11-12T00:00:00.000Z
URL: https://www.metaculus.com/questions/578/human-extinction-by-2100/?invite=oLVbaM
Content: Human Extinction by 2100?
[# ](https://www.metaculus.com/)
* [Questions](https://www.metaculus.com/questions/)
* [Tournaments](https://www.metaculus.com/tournaments/)
* [Services](https://www.metaculus.com/services/)
* [News](https://www.metaculus.com/news/)
* [Questions](https://www.metaculus.com/q

[A Global Catastrophe This Century]
Date: 2022-06-29T00:00:00.000Z
URL: https://www.metaculus.com/notebooks/8736/a-global-catastrophe-this-century/
Content: A Global Catastrophe This Century
**The Bridgewater x Metaculus Forecasting Contest**returns for year three. $30,000 in prizes across Open and Undergraduate competitions, plus recruiting opportunities for top forecasters. Past contests have led to internship offers and full-time hires at Bridgewater

[CTs AI Extinction Before 2100]
Date: 2024-08-11T00:00:00.000Z
URL: https://www.metaculus.com/questions/27035/
Content: Question

# Before 2100, will AI cause the human population to fall below 5000 individuals?

3

[9 comments9](https://www.metaculus.com/questions/27035/cts-ai-extinction-before-2100/#comments)

1d1w2mall

Total Forecasters73

Community Prediction
4%

(1% - 32%)

### Make a Prediction

50%

community

[How Likely Are Various Precursors of Existential Risk?]
Date: 2024-10-22T00:00:00.000Z
URL: https://blog.sentinel-team.org/p/how-likely-are-various-precursors
Content: # [Sentinel Global Risks Watch](https://blog.sentinel-team.org/)

SubscribeSign in

# How Likely Are Various Precursors of Existential Risk?

### War (23%), natural pandemic (22%), the unexpected (21%), WW3 specifically (12%), AI (10%), ..., US civil war (0.92%), solar storm (0.2%), asteroid impact 

[How Likely Are Various Precursors of Existential   Risk?]
Date: 2024-10-22T00:00:00.000Z
URL: https://forum.effectivealtruism.org/posts/KZXkP9Qs8Sy8AWcpp/how-likely-are-various-precursors-of-existential-risk
Content: How Likely Are Various Precursors of Existential Risk? ‚ÄîEA Forum
## [
![Effective Altruism Forum Logo](https://images.ctfassets.net/ohf186sfn6di/7J4cBC9SXCWMoqqCIqI0GI/affe205261bb8cff47501a0ada0f2268/ea-logo-square-1200x1200__1_.png?h=100 "Effective Altruism Forum")
Effective Altruism Forum](https:

=== CRAWL_URLS RESULTS ===

[Human Extinction by 2100?]
URL: https://www.metaculus.com/questions/578/human-extinction-by-2100/
Content: [**371** comments](https://www.metaculus.com/questions/578/human-extinction-by-2100/#comments)

**1.7k** forecasters

# Will humans go extinct before 2100?

1%chance

The community sees this as a toss-up.

Share

Predict

Forecast Timeline

1d1w2mall

CommentsTimelineQuestion Info

Resolution Criteria

Key Factors

Background Info

Histogram

Followembed

Authors:

[jzima](https://www.metaculus.com/accounts/profile/103733/)

Opened:Nov 12, 2017

Closes:Jan 2, 2100

Scheduled resolution:Jan 2, 21

[Before 2100, will AI cause the human population to fall below 5000 individuals?]
URL: https://www.metaculus.com/questions/27035/cts-ai-extinction-before-2100/
Content: 
 
 
 
Make a Prediction
 
 
 


=== SEARCH_WEB RESULTS ===

[How many lives does the future hold? | Global Priorities Institute]
Date: None
URL: https://www.globalprioritiesinstitute.org/wp-content/uploads/Toby-Newberry_How-many-lives-does-the-future-hold.pdf
Content: How many lives does the
future hold?
Toby Newberry
Global Priorities Institute | May 2021
GPI Technical Report No . T2-2021
How many lives does the future hold?
1*
1. Introduction
The total number of people who have ever lived, acrossthe entire human past, has been estimated at
around 100 billion.
2

[Human extinction - Wikipedia]
Date: None
URL: https://en.wikipedia.org/wiki/Human_extinction
Content: Human extinction - Wikipedia
[Jump to content](#bodyContent)
[![](https://en.wikipedia.org/static/images/icons/wikipedia.png)![Wikipedia](https://en.wikipedia.org/static/images/mobile/copyright/wikipedia-wordmark-en.svg)![The Free Encyclopedia](https://en.wikipedia.org/static/images/mobile/copyright

[Human Extinction by 2100? - Metaculus]
Date: None
URL: https://www.metaculus.com/questions/578/human-extinction-by-2100/
Content: [**371** comments](https://www.metaculus.com/questions/578/human-extinction-by-2100/#comments)

**1.7k** forecasters

# Will humans go extinct before 2100?

1%chance

The community sees this as a toss-up.

Share

Predict

Forecast Timeline

1d1w2mall

CommentsTimelineQuestion Info

Resolution Criter

[Global Population Decline >10% by 2100 - Metaculus]
Date: None
URL: https://www.metaculus.com/questions/1493/global-population-decline-10-by-2100/
Content: Question

59

[20 comments20](https://www.metaculus.com/questions/1493/by-2100-will-the-human-population-decrease-by-at-least-10-during-any-period-of-5-years/#comments)

Total Forecasters394

Community Prediction

35%

(15% - 67%)

### Make a Prediction

50%

community: 35%

No key factors yetAdd so

[The Economics of AI Apocalypse | AEI - American Enterprise Institute]
Date: 2025-03-19T20:30:22.000Z
URL: https://www.aei.org/articles/the-economics-of-ai-apocalypse/
Content: [![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7645922b-e743-4b7a-9213-07a2891a7e11_1451x815.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/http

=== SEARCH_WEB RESULTS ===

[An unusual way to figure out if humanity is toast - Vox]
Date: 2023-07-10T00:00:00.000Z
URL: https://www.vox.com/future-perfect/23785731/human-extinction-forecasting-superforecasters
Content: [Skip to main content](https://www.vox.com/www.vox.com#content)

[The homepageVox logo](https://www.vox.com/)

Menu

[The homepageVox logo](https://www.vox.com/)

Navigation Drawer

- [Login](https://www.vox.com/auth/login?returnPath=/)
/
[Sign Up](https://www.vox.com/auth/login?returnPath=/)

close

[Ezra Karger on what superforecasters and experts think ...]
Date: 2025-10-13T15:51:15.000Z
URL: https://80000hours.org/podcast/episodes/ezra-karger-forecasting-existential-risks/
Content: ## On this page:

- [Introduction](https://80000hours.org/80000hours.org#top)
- [1 Highlights](https://80000hours.org/80000hours.org#highlights)
- [2 Articles, books, and other media discussed in the show](https://80000hours.org/80000hours.org#articles-books-and-other-media-discussed-in-the-show)
- 

[Announcing "Forecasting Existential Risks: Evidence from ...]
Date: 2023-07-10T00:00:00.000Z
URL: https://forum.effectivealtruism.org/posts/un42vaZgyX7ch2kaj/announcing-forecasting-existential-risks-evidence-from-a
Content: Announcing &quot;Forecasting Existential Risks: Evidence from a Long-Run Forecasting Tournament&quot; ‚ÄîEA Forum
## [
![Effective Altruism Forum Logo](https://images.ctfassets.net/ohf186sfn6di/7J4cBC9SXCWMoqqCIqI0GI/affe205261bb8cff47501a0ada0f2268/ea-logo-square-1200x1200__1_.png?h=100 "Effective Al

[Forecasting the End of the World - by Robert de Neufville]
Date: 2024-01-08T23:46:27.351Z
URL: https://tellingthefuture.substack.com/p/forecasting-the-end-of-the-world
Content: # [Telling the Future](https://tellingthefuture.substack.com/)

SubscribeSign in

# Forecasting the End of the World

### New estimates of p(doom)

[Robert de Neufville](https://substack.com/@tellingthefuture)

Jul 25, 2023

12

[15](https://tellingthefuture.substack.com/p/forecasting-the-end-of-the

[A tournament tried to test how well experts could forecast AI progress. They were all wrong.]
Date: 2025-09-05T00:00:00.000Z
URL: https://www.vox.com/future-perfect/460222/ai-forecasting-tournament-superforecaster-expert-tetlock
Content: [Skip to main content](https://www.vox.com/www.vox.com#content)

[The homepageVox logo](https://www.vox.com/)

Menu

[The homepageVox logo](https://www.vox.com/)

Navigation Drawer

- [Login](https://www.vox.com/auth/login?returnPath=/)
/
[Sign Up](https://www.vox.com/auth/login?returnPath=/)

close

Today is 2026-01-08.

=== INTEGRATIVE REASONING ===

Your research assistant has done the preliminary research for you. Your task is NOT to re-summarize the data, but to **coalesce your own thoughts** and use your specialized ability to **piece together the information in an integrated way**. 

Avoid rambling. Focus on the final synthesis:
(a) How do these specific facts interact to change the probability?
(b) What is the most likely "integrated" scenario?
(c) What is your final, calibrated judgment?

=== ANALYSIS FRAMEWORK ===

Before answering, work through:
(a) Time remaining until resolution - does this affect the probability?
(b) Status quo outcome if nothing changes
(c) Scenario that results in a NO outcome
(d) Scenario that results in a YES outcome
(e) What do prediction markets, experts, or Metaculus community suggest?
(f) What's your uncertainty? Is this a question where good and bad forecasters differ, or is it straightforward?
(g) **Causal Analysis**: List 5 specific causal links with a direct known connection to the outcome and rate their relevance.
(h) **Missing Information**: What critical data are you missing? How does this gap affect your prior?

=== YOUR FORECAST ===

The last thing you write is your final answer as: "Probability: ZZ%", 0-100

```

</details>
